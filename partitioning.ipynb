{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/05/09 13:28:09 WARN Utils: Your hostname, Sandevistan resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/09 13:28:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 13:28:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/09 13:28:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded raw rows: 1507350137\n",
      "root\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- quote_asset_volume: float (nullable = true)\n",
      " |-- number_of_trades: integer (nullable = true)\n",
      " |-- taker_buy_base_asset_volume: float (nullable = true)\n",
      " |-- taker_buy_quote_asset_volume: float (nullable = true)\n",
      " |-- open_time: timestamp_ntz (nullable = true)\n",
      " |-- symbol: string (nullable = false)\n",
      "\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------+\n",
      "|open   |high   |low    |close  |volume  |quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume|open_time          |symbol  |\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------+\n",
      "|4261.48|4261.48|4261.48|4261.48|1.775183|7564.9067         |3               |0.075183                   |320.39084                   |2017-08-17 04:00:00|BTC-USDT|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.0     |0.0               |0               |0.0                        |0.0                         |2017-08-17 04:01:00|BTC-USDT|\n",
      "|4280.56|4280.56|4280.56|4280.56|0.261074|1117.543          |2               |0.261074                   |1117.543                    |2017-08-17 04:02:00|BTC-USDT|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.012008|51.171852         |3               |0.012008                   |51.171852                   |2017-08-17 04:03:00|BTC-USDT|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.140796|599.9993          |1               |0.140796                   |599.9993                    |2017-08-17 04:04:00|BTC-USDT|\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract\n",
    "\n",
    "os.environ.setdefault(\"JAVA_HOME\", \"/usr/lib/jvm/java-11-openjdk-amd64\")\n",
    "os.environ.setdefault(\"SPARK_HOME\", \"/opt/spark\")\n",
    "\n",
    "# 1) Start Spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"CryptoPreprocess\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        .config(\"spark.executor.memory\", \"8g\")\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "base_dir = \"archive\"\n",
    "\n",
    "parquet_files = []\n",
    "for root, _, files in os.walk(base_dir):\n",
    "    for fn in files:\n",
    "        if fn.lower().endswith(\".parquet\"):\n",
    "            parquet_files.append(os.path.join(root, fn))\n",
    "\n",
    "# 2) Read just those parquet paths & extract symbol\n",
    "raw = spark.read.parquet(*parquet_files)\n",
    "raw = raw.withColumn(\n",
    "    \"symbol\",\n",
    "    regexp_extract(input_file_name(), r\"([^/\\\\]+)\\.parquet$\", 1)\n",
    ")\n",
    "\n",
    "print(\"✅ Loaded raw rows:\", raw.count())\n",
    "raw.printSchema()\n",
    "raw.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c109529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Computed daily_return & volatility — rows: 1507350137\n",
      "+--------+----------+------------+----------+\n",
      "|symbol  |date      |daily_return|volatility|\n",
      "+--------+----------+------------+----------+\n",
      "|BTC-USDT|2017-08-17|0.0         |0.0       |\n",
      "|BTC-USDT|2017-08-17|0.0         |0.0       |\n",
      "|BTC-USDT|2017-08-17|0.0         |0.0       |\n",
      "|BTC-USDT|2017-08-17|0.0         |0.0       |\n",
      "|BTC-USDT|2017-08-17|0.0         |0.0       |\n",
      "+--------+----------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# 2.1) Convert the existing timestamp to a simple date\n",
    "enhanced = raw.withColumn(\"date\", to_date(col(\"open_time\")))\n",
    "\n",
    "# 2.2) Drop the raw timestamp & unused columns, filter out nulls\n",
    "processed = (\n",
    "    enhanced\n",
    "    .drop(\n",
    "        \"open_time\",\n",
    "        \"quote_asset_volume\",\n",
    "        \"number_of_trades\",\n",
    "        \"taker_buy_base_asset_volume\",\n",
    "        \"taker_buy_quote_asset_volume\"\n",
    "    )\n",
    "    .dropna(subset=[\"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    ")\n",
    "\n",
    "# 2.3) Compute daily_return and volatility\n",
    "processed = (\n",
    "    processed\n",
    "    .withColumn(\"daily_return\", (col(\"close\") - col(\"open\")) / col(\"open\"))\n",
    "    .withColumn(\"volatility\",    (col(\"high\")  - col(\"low\"))  / col(\"open\"))\n",
    ")\n",
    "\n",
    "print(\"✅ Computed daily_return & volatility — rows:\", processed.count())\n",
    "processed.select(\"symbol\", \"date\", \"daily_return\", \"volatility\") \\\n",
    "         .show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(342 + 1) / 343]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  Repartitioned — partitions: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=====================================================>(342 + 1) / 343]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------------------+-------------------+\n",
      "|symbol  |date      |daily_return         |volatility         |\n",
      "+--------+----------+---------------------+-------------------+\n",
      "|HNT-USDT|2020-09-24|2.42688166895055     |3.3010751716469553 |\n",
      "|HNT-USDT|2020-09-24|-0.2643551809882768  |0.318638235681588  |\n",
      "|HNT-USDT|2020-09-24|-0.039880629022328265|0.1251866824032755 |\n",
      "|HNT-USDT|2020-09-24|-0.14504662349076586 |0.16792532827305506|\n",
      "|HNT-USDT|2020-09-24|0.03261718060488578  |0.1110535620341333 |\n",
      "+--------+----------+---------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cell 3: Repartition\n",
    "\n",
    "# 3.1) Repartition by symbol for downstream parallelism\n",
    "processed = processed.repartition(\"symbol\")\n",
    "print(\"ℹ️  Repartitioned — partitions:\", processed.rdd.getNumPartitions())\n",
    "\n",
    "# 3.2) Peek at a tiny sample to verify everything’s still in order\n",
    "processed.select(\"symbol\", \"date\", \"daily_return\", \"volatility\") \\\n",
    "         .limit(5) \\\n",
    "         .show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
