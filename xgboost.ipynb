{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3a1187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/oliverzheng/spark-3.5.3-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oliverzheng/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oliverzheng/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "ml.dmlc#xgboost4j-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-57ebf77f-5548-46f7-a429-500f469096e9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.568 in central\n",
      "\tfound ml.dmlc#xgboost4j-spark_2.12;1.5.1 in central\n",
      "\tfound ml.dmlc#xgboost4j_2.12;1.5.1 in central\n",
      "\tfound com.typesafe.akka#akka-actor_2.12;2.5.23 in central\n",
      "\tfound com.typesafe#config;1.3.3 in central\n",
      "\tfound org.scala-lang.modules#scala-java8-compat_2.12;0.8.0 in central\n",
      "\tfound com.esotericsoftware#kryo;4.0.2 in central\n",
      "\tfound com.esotericsoftware#reflectasm;1.11.3 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in local-m2-cache\n",
      "\tfound com.esotericsoftware#minlog;1.3.0 in central\n",
      "\tfound org.objenesis#objenesis;2.5.1 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.8 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in local-m2-cache\n",
      ":: resolution report :: resolve 470ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.568 from central in [default]\n",
      "\tcom.esotericsoftware#kryo;4.0.2 from central in [default]\n",
      "\tcom.esotericsoftware#minlog;1.3.0 from central in [default]\n",
      "\tcom.esotericsoftware#reflectasm;1.11.3 from central in [default]\n",
      "\tcom.typesafe#config;1.3.3 from central in [default]\n",
      "\tcom.typesafe.akka#akka-actor_2.12;2.5.23 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from local-m2-cache in [default]\n",
      "\tml.dmlc#xgboost4j-spark_2.12;1.5.1 from central in [default]\n",
      "\tml.dmlc#xgboost4j_2.12;1.5.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.objenesis#objenesis;2.5.1 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from local-m2-cache in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-java8-compat_2.12;0.8.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.568] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-57ebf77f-5548-46f7-a429-500f469096e9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/10ms)\n",
      "25/05/09 00:50:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 00:50:35 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------------------+-----------+\n",
      "|   open|   high|    low|  close|  volume|quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume|          open_time|         source_file|crypto_name|\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------------------+-----------+\n",
      "|4261.48|4261.48|4261.48|4261.48|1.775183|         7564.9067|               3|                   0.075183|                   320.39084|2017-08-17 04:00:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:01:00|s3a://cryptospark...|        BTC|\n",
      "|4280.56|4280.56|4280.56|4280.56|0.261074|          1117.543|               2|                   0.261074|                    1117.543|2017-08-17 04:02:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.012008|         51.171852|               3|                   0.012008|                   51.171852|2017-08-17 04:03:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.140796|          599.9993|               1|                   0.140796|                    599.9993|2017-08-17 04:04:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:05:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:06:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:07:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:08:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:09:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:10:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:11:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:12:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:13:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:14:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:15:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:16:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4264.88|4261.48|4264.88|0.075455|         321.60333|               2|                   0.075455|                   321.60333|2017-08-17 04:17:00|s3a://cryptospark...|        BTC|\n",
      "|4264.88|4264.88|4264.88|4264.88|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:18:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.409211|         1743.8445|               7|                       0.01|                     42.6148|2017-08-17 04:19:00|s3a://cryptospark...|        BTC|\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import input_file_name, col, regexp_extract\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "\n",
    "# Step 1: Initialize SparkSession with S3 support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 USDT Parquet Reader\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.568,\"\n",
    "            \"ml.dmlc:xgboost4j-spark_2.12:1.5.1\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIA2WFFOP3DIZ7VCICR\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"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\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Read all files with \"USDT\" in the name\n",
    "s3_path = \"s3a://cryptospark-dataset/archive/*USDT*.parquet\"\n",
    "\n",
    "df = spark.read.parquet(s3_path)\n",
    "\n",
    "def extract_crypto_name(df):\n",
    "    df_with_crypto = df.withColumn(\n",
    "        \"crypto_name\",\n",
    "        F.regexp_extract(F.col(\"source_file\"), r\"\\/([^\\/]+)-USDT\\.parquet\", 1)\n",
    "    )\n",
    "    return df_with_crypto\n",
    "df_with_path = (\n",
    "    spark.read.parquet(\"s3a://cryptospark-dataset/archive/BTC-USDT.parquet\")\n",
    "         .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "df_with_crypto = extract_crypto_name(df_with_path)\n",
    "df_with_crypto.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2342d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015a8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba256af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- quote_asset_volume: float (nullable = true)\n",
      " |-- number_of_trades: integer (nullable = true)\n",
      " |-- taker_buy_base_asset_volume: float (nullable = true)\n",
      " |-- taker_buy_quote_asset_volume: float (nullable = true)\n",
      " |-- open_time: timestamp_ntz (nullable = true)\n",
      " |-- source_file: string (nullable = false)\n",
      " |-- crypto_name: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------------------+-----------+\n",
      "|   open|   high|    low|  close|  volume|quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume|          open_time|         source_file|crypto_name|\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------------------+-----------+\n",
      "|4261.48|4261.48|4261.48|4261.48|1.775183|         7564.9067|               3|                   0.075183|                   320.39084|2017-08-17 04:00:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:01:00|s3a://cryptospark...|        BTC|\n",
      "|4280.56|4280.56|4280.56|4280.56|0.261074|          1117.543|               2|                   0.261074|                    1117.543|2017-08-17 04:02:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.012008|         51.171852|               3|                   0.012008|                   51.171852|2017-08-17 04:03:00|s3a://cryptospark...|        BTC|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.140796|          599.9993|               1|                   0.140796|                    599.9993|2017-08-17 04:04:00|s3a://cryptospark...|        BTC|\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df_with_crypto.orderBy(\"open_time\")\n",
    "\n",
    "# Show the schema and data\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511df85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 2202546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data count: 550636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Add a long timestamp column to df_with_crypto\n",
    "df_with_ts = df_with_crypto.withColumn(\"open_time_long\", F.unix_timestamp(\"open_time\"))\n",
    "\n",
    "# Step 2: Compute the 80% quantile based on the long timestamp\n",
    "split_time = df_with_ts.approxQuantile(\"open_time_long\", [0.8], 0.0)[0]\n",
    "\n",
    "# Step 3: Split the data using this timestamp threshold\n",
    "train_data = df_with_ts.filter(F.col(\"open_time_long\") <= split_time)\n",
    "test_data = df_with_ts.filter(F.col(\"open_time_long\") > split_time)\n",
    "\n",
    "# Print counts\n",
    "print(f\"Training data count: {train_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671dd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.orderBy(\"open_time_long\")\n",
    "train_data = train_data.withColumn(\"lag1\", F.lag(\"close\", 1).over(window_spec))\n",
    "test_data = test_data.withColumn(\"lag1\", F.lag(\"close\", 1).over(window_spec))\n",
    "train_data = train_data.dropna(subset=[\"lag1\"])\n",
    "test_data = test_data.dropna(subset=[\"lag1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5773d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"lag1\"], outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data)\n",
    "test_data = assembler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88c75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 00:52:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------+------------------+\n",
      "|open_time_long|  close|   lag1|          features|\n",
      "+--------------+-------+-------+------------------+\n",
      "|    1502942460|4261.48|4261.48|[4261.47998046875]|\n",
      "|    1502942520|4280.56|4261.48|[4261.47998046875]|\n",
      "|    1502942580|4261.48|4280.56|[4280.56005859375]|\n",
      "|    1502942640|4261.48|4261.48|[4261.47998046875]|\n",
      "|    1502942700|4261.48|4261.48|[4261.47998046875]|\n",
      "+--------------+-------+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.select(\"open_time_long\", \"close\", \"lag1\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099b32db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 00:52:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:52:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:53:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:53:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:53:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 00:54:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "/Users/oliverzheng/anaconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [00:54:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 717.1838691036731\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data for XGBoost (converting features into numpy arrays)\n",
    "def prepare_data_for_xgboost(df):\n",
    "    features_col = df.select(\"features\").rdd.map(lambda row: row['features'].toArray()).collect()\n",
    "    label_col = df.select(\"close\").rdd.map(lambda row: row['close']).collect()\n",
    "    return np.array(features_col), np.array(label_col)\n",
    "\n",
    "# Prepare training and test data\n",
    "X_train, y_train = prepare_data_for_xgboost(train_data)\n",
    "X_test, y_test = prepare_data_for_xgboost(test_data)\n",
    "\n",
    "# Train XGBoost model\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# Evaluate the model (RMSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01e1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = spark.createDataFrame([(float(pred),) for pred in y_pred], [\"prediction\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b2a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/09 01:02:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 01:02:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 01:02:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 01:02:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 01:02:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/05/09 01:02:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: 0.7045395335215743%\n",
      "Accuracy: 99.29546046647843%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Convert the `y_pred` (predictions) into a DataFrame with one column of predictions\n",
    "predictions_list = [float(pred) for pred in y_pred]\n",
    "pred_df = spark.createDataFrame([(float(pred),) for pred in predictions_list], [\"prediction\"])\n",
    "\n",
    "# Step 2: Add an index to both `test_data` and `pred_df` for row-wise alignment\n",
    "test_data_with_index = test_data.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "pred_df_with_index = pred_df.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Step 3: Perform a join based on the index (row_id) to match predictions with the original test data\n",
    "test_data_with_preds = test_data_with_index.join(pred_df_with_index, on=\"row_id\").drop(\"row_id\")\n",
    "\n",
    "# Step 4: Calculate the absolute percentage error for each row\n",
    "df_with_error = test_data_with_preds.withColumn(\n",
    "    \"abs_percentage_error\", \n",
    "    abs((col(\"close\") - col(\"prediction\")) / col(\"close\")) * 100\n",
    ")\n",
    "\n",
    "# Step 5: Calculate the Mean Absolute Percentage Error (MAPE)\n",
    "mape = df_with_error.agg({\"abs_percentage_error\": \"mean\"}).collect()[0][0]\n",
    "\n",
    "# Step 6: Calculate the accuracy (100% - MAPE)\n",
    "accuracy = 100 - mape\n",
    "\n",
    "print(f\"MAPE: {mape}%\")\n",
    "print(f\"Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ed384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
