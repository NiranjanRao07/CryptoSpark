# CryptoSpark Demo

A minimal end-to-end example showing how to:

1. **Process** large parquet trade files with PySpark  
2. **Engineer** features (daily return, volatility, moving averages)  
3. **Train** a logistic-regression model  
4. **Serve** predictions via a Flask API  
5. **Interact** with a Streamlit dashboard

---

## ğŸ“¦ Repository Structure

```

.
â”œâ”€â”€ data/                      # Raw & processed parquet files
â”œâ”€â”€ models/                    # Saved Spark ML `PipelineModel`
â”œâ”€â”€ api.py                     # Flask app serving `/predict` endpoint
â”œâ”€â”€ app.py                     # Streamlit dashboard
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file

````

---

## ğŸš€ Quick Start

### 1. Install dependencies

Create & activate a virtual environment, then:

```bash
pip install --break-system-packages -r requirements.txt
````

### 2. Preprocess & train (optional)

If you need to regenerate features or retrain:

```bash
# Launch PySpark shell or run your notebook/script:
python preprocess_and_train.py
# â†’ produces `data/processed-data/` and `models/logistic.pipeline`
```

### 3. Run the Flask API

```bash
export FLASK_APP=api.py
python -m flask run
```

By default youâ€™ll have a server at `http://127.0.0.1:5000/predict`.

### 4. Launch the Streamlit UI

```bash
streamlit run app.py
```

Interactively explore your modelâ€™s predictions in the browser.

---

## ğŸ”§ Configuration

* **Spark**: configured in `api.py` / `app.py` via `SparkSession.builder`.
* **Model**: loaded with `PipelineModel.load("models/logistic.pipeline")`.
* **Flask**: simple JSON POST `/predict` â†’ `{ features: [â€¦] }`.
* **Streamlit**: reads user inputs, calls Flask API, displays results.

---

## ğŸ¤ Contributing

1. Fork this repo
2. Create a feature branch
3. Commit & push your changes
4. Open a pull request

---
