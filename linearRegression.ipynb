{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b4c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_date, first, last, max as _max, min as _min, sum as _sum, avg, \n",
    "    input_file_name, regexp_extract, col, log, exp\n",
    ")\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b3a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with necessary JARs for S3 access\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LinearRegressionModel\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.568\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"com.amazonaws.auth.profile.ProfileCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d9363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- volume: float (nullable = true)\n",
      " |-- quote_asset_volume: float (nullable = true)\n",
      " |-- number_of_trades: integer (nullable = true)\n",
      " |-- taker_buy_base_asset_volume: float (nullable = true)\n",
      " |-- taker_buy_quote_asset_volume: float (nullable = true)\n",
      " |-- open_time: timestamp_ntz (nullable = true)\n",
      " |-- symbol: string (nullable = false)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------+----------+\n",
      "|   open|   high|    low|  close|  volume|quote_asset_volume|number_of_trades|taker_buy_base_asset_volume|taker_buy_quote_asset_volume|          open_time|  symbol|      date|\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------+----------+\n",
      "|4261.48|4261.48|4261.48|4261.48|1.775183|         7564.9067|               3|                   0.075183|                   320.39084|2017-08-17 04:00:00|BTC-USDT|2017-08-17|\n",
      "|4261.48|4261.48|4261.48|4261.48|     0.0|               0.0|               0|                        0.0|                         0.0|2017-08-17 04:01:00|BTC-USDT|2017-08-17|\n",
      "|4280.56|4280.56|4280.56|4280.56|0.261074|          1117.543|               2|                   0.261074|                    1117.543|2017-08-17 04:02:00|BTC-USDT|2017-08-17|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.012008|         51.171852|               3|                   0.012008|                   51.171852|2017-08-17 04:03:00|BTC-USDT|2017-08-17|\n",
      "|4261.48|4261.48|4261.48|4261.48|0.140796|          599.9993|               1|                   0.140796|                    599.9993|2017-08-17 04:04:00|BTC-USDT|2017-08-17|\n",
      "+-------+-------+-------+-------+--------+------------------+----------------+---------------------------+----------------------------+-------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"C:/Users/dumas/OneDrive/Documents/School/SJSU/DATA 228/dataset\")\n",
    "\n",
    "# Extract symbol from filename (e.g., 1INCH-BTC.parquet), and date from open_time column\n",
    "df = df.withColumn(\"symbol\", regexp_extract(input_file_name(), r\"([^/]+)\\.parquet$\", 1)) \\\n",
    "    .withColumn(\"date\", to_date(\"open_time\"))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49b786",
   "metadata": {},
   "source": [
    "# Preprocessing 0.1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f6ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "processed_df = df.drop(\"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"open_time\")\n",
    "\n",
    "# Drop rows with nulls in critical columns\n",
    "processed_df = processed_df.dropna(subset=[\"open\", \"high\", \"low\", \"close\", \"volume\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744ad12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1506508\n"
     ]
    }
   ],
   "source": [
    "sample_df = processed_df.sample(fraction=0.001, seed=42)\n",
    "print(\"Rows:\", sample_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00114eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition the DataFrame by symbol to optimize for groupBy operations\n",
    "sample_df = sample_df.repartition(\"symbol\")\n",
    "\n",
    "sample_df = sample_df.groupBy(\"symbol\", \"date\").agg(\n",
    "    first(\"open\").alias(\"open\"),\n",
    "    last(\"close\").alias(\"close\"),\n",
    "    _max(\"high\").alias(\"high\"),\n",
    "    _min(\"low\").alias(\"low\"),\n",
    "    _sum(\"volume\").alias(\"volume\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55576237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional features, like daily return and volatility\n",
    "sample_df = sample_df.withColumn(\"daily_return\", (col(\"close\") - col(\"open\")) / col(\"open\")) \\\n",
    "       .withColumn(\"volatility\", (col(\"high\") - col(\"low\")) / col(\"open\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6263711",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.repartition(\"symbol\")\n",
    "\n",
    "# Cumulative return\n",
    "window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Moving Averages (7-day and 30-day)\n",
    "window_7 = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "window_30 = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-29, 0)\n",
    "\n",
    "# Calculate cumulative return and moving averages\n",
    "sample_df = sample_df.withColumn(\"ma_7\", avg(\"close\").over(window_7)) \\\n",
    "    .withColumn(\"ma_30\", avg(\"close\").over(window_30)) \\\n",
    "    .withColumn(\"cumulative_return\", exp(_sum(log(1 + col(\"daily_return\"))).over(window_spec)) - 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a9e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with nulls in critical columns\n",
    "sample_df = sample_df.dropna(subset=[\"ma_7\", \"ma_30\", \"cumulative_return\", \"daily_return\", \"volatility\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.cache()\n",
    "sample_df.show(5)  # Materialize cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1308d6",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05110d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df535df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the target variable to \"label\"\n",
    "final_df = sample_df.withColumnRenamed(\"daily_return\", \"label\")\n",
    "final_df.printSchema\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b162e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and string index the categorical features\n",
    "symbol_indexer = StringIndexer(inputCol=\"symbol\", outputCol=\"symbol_index\", handleInvalid=\"skip\")\n",
    "symbol_encoder = OneHotEncoder(inputCols=[\"symbol_index\"], outputCols=[\"symbol_ohe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da162d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for modeling\n",
    "numeric_features = ['open', 'high', 'low', 'close', 'volume', \n",
    "                    'volatility', 'ma_7', 'ma_30', 'cumulative_return']\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=numeric_features + ['symbol_ohe'], outputCol=\"features\")\n",
    "final_df = assembler.transform(final_df).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Pipeline for the model\n",
    "pipeline = Pipeline(stages=[symbol_indexer, symbol_encoder, assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ebf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear regression model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "results = model.transform(test_data)\n",
    "summary = model.stages[-1].summary\n",
    "print(\"RMSE:\", summary.rootMeanSquaredError)\n",
    "print(\"R2:\", summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa981363",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convrert results to Pandas DataFrame for easier manipulation\n",
    "pandas_df = results.select(\"label\", \"prediction\").sample(fraction=0.1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0048c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of actual vs predicted daily return\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(pandas_df[\"label\"], pandas_df[\"prediction\"], alpha=0.5)\n",
    "plt.xlabel(\"Actual Daily Return\")\n",
    "plt.ylabel(\"Predicted Daily Return\")\n",
    "plt.title(\"Predicted vs Actual Daily Return\")\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae475ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of distribution of actual vs predicted daily return\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(pandas_df[\"label\"], bins=50, alpha=0.6, label=\"Actual\", color='blue')\n",
    "plt.hist(pandas_df[\"prediction\"], bins=50, alpha=0.6, label=\"Predicted\", color='orange')\n",
    "plt.xlabel(\"Daily Return\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram: Actual vs Predicted Returns\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5caafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "pandas_df[\"residual\"] = pandas_df[\"prediction\"] - pandas_df[\"label\"]\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Residual Mean:\", pandas_df[\"residual\"].mean())\n",
    "print(\"Residual Std Dev:\", pandas_df[\"residual\"].std())\n",
    "print(\"Residual Min:\", pandas_df[\"residual\"].min())\n",
    "print(\"Residual Max:\", pandas_df[\"residual\"].max())\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(pandas_df[\"label\"], pandas_df[\"residual\"], alpha=0.5)\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.xlabel(\"Actual Daily Return\")\n",
    "plt.ylabel(\"Residual (Prediction - Actual)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20acc50",
   "metadata": {},
   "source": [
    "# Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "model_path = \"C:/Users/dumas/OneDrive/Documents/GitHub/CryptoSpark/linearRegression_model\"\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
