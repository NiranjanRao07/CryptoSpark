{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    to_date, first, last, max as _max, min as _min, sum as _sum, avg, \n",
    "    input_file_name, regexp_extract, col, log, exp\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with necessary JARs for S3 access\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from S3\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.568\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "            \"com.amazonaws.auth.profile.ProfileCredentialsProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parquet data from S3\n",
    "\n",
    "df = spark.read.parquet(\"s3a://cryptospark-dataset/archive/\")\n",
    "\n",
    "# Extract symbol from filename (e.g., 1INCH-BTC.parquet), and date from open_time column\n",
    "df = df.withColumn(\"symbol\", regexp_extract(input_file_name(), r\"([^/]+)\\.parquet$\", 1)) \\\n",
    "    .withColumn(\"date\", to_date(\"open_time\"))\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for key numeric columns\n",
    "df.describe([\"open\", \"high\", \"low\", \"close\", \"volume\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Range \n",
    "df.select(_min(\"open_time\").alias(\"start_date\"), _max(\"open_time\").alias(\"end_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to daily OHLCV\n",
    "daily_df = df.groupBy(\"date\").agg(\n",
    "    first(\"open\").alias(\"open\"),\n",
    "    last(\"close\").alias(\"close\"),\n",
    "    _max(\"high\").alias(\"high\"),\n",
    "    _min(\"low\").alias(\"low\"),\n",
    "    avg(\"close\").alias(\"avg_close\"),\n",
    "    _sum(\"volume\").alias(\"total_volume\")\n",
    ").orderBy(\"date\")\n",
    "\n",
    "# Cache the result for faster access in future steps (avoiding recomputation)\n",
    "daily_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the data based on 'date' to improve parallelism before the window operation\n",
    "repartitioned_df = daily_df.repartition(100, \"date\")  # Adjust the number of partitions as needed\n",
    "\n",
    "# Define window specification: partition by 'date' (or 'symbol' if you have multiple symbols)\n",
    "window_spec = Window.partitionBy(\"date\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "\n",
    "# Add 7-day moving averages for 'close' and 'volume'\n",
    "daily_df = repartitioned_df.withColumn(\"ma_close\", avg(\"close\").over(window_spec)) \\\n",
    "                           .withColumn(\"ma_volume\", avg(\"total_volume\").over(window_spec))\n",
    "\n",
    "# Cache the results again to speed up future accesses\n",
    "daily_df.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check row count to ensure safe Pandas conversion\n",
    "print(\"Daily count:\", daily_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid memory issues, only convert the aggregated daily data to Pandas\n",
    "# Check data size first to avoid running out of memory\n",
    "daily_pd = daily_df.limit(2000).toPandas()  # Limit to the first 2000 rows for visualization\n",
    "daily_pd[\"date\"] = pd.to_datetime(daily_pd[\"date\"])\n",
    "daily_pd = daily_pd.sort_values(\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the figure layout\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 18), sharex=True)\n",
    "\n",
    "# Plot Open Price\n",
    "axes[0].plot(daily_pd[\"date\"], daily_pd[\"open\"], label=\"Open\", color=\"blue\", alpha=0.7)\n",
    "axes[0].set_title(\"Daily Open Price\")\n",
    "axes[0].set_ylabel(\"Price\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot Close Price\n",
    "axes[1].plot(daily_pd[\"date\"], daily_pd[\"close\"], label=\"Close\", color=\"green\", alpha=0.7)\n",
    "axes[1].set_title(\"Daily Close Price\")\n",
    "axes[1].set_ylabel(\"Price\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Plot High Price\n",
    "axes[2].plot(daily_pd[\"date\"], daily_pd[\"high\"], label=\"High\", color=\"orange\", alpha=0.7)\n",
    "axes[2].set_title(\"Daily High Price\")\n",
    "axes[2].set_ylabel(\"Price\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "# Plot Low Price\n",
    "axes[3].plot(daily_pd[\"date\"], daily_pd[\"low\"], label=\"Low\", color=\"red\", alpha=0.7)\n",
    "axes[3].set_title(\"Daily Low Price\")\n",
    "axes[3].set_ylabel(\"Price\")\n",
    "axes[3].legend()\n",
    "axes[3].grid(True)\n",
    "\n",
    "# Plot Avg Close\n",
    "axes[4].plot(daily_pd[\"date\"], daily_pd[\"avg_close\"], label=\"Average Close\", color=\"purple\", linestyle=\"--\")\n",
    "axes[4].set_title(\"Daily Average Close Price\")\n",
    "axes[4].set_xlabel(\"Date\")\n",
    "axes[4].set_ylabel(\"Price\")\n",
    "axes[4].legend()\n",
    "axes[4].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pd = daily_pd.dropna(subset=[\"close\", \"ma_close\"])\n",
    "\n",
    "# Plot: Daily Close with 7-day Moving Average\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(daily_pd[\"date\"], daily_pd[\"close\"], label=\"Close Price\", alpha=0.6)\n",
    "plt.plot(daily_pd[\"date\"], daily_pd[\"ma_close\"], label=\"7-Day MA\", color=\"red\")\n",
    "plt.title(\"Daily Close Price and 7-Day Moving Average\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot: Daily Volume with 7-day Moving Average\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(daily_pd[\"date\"], daily_pd[\"total_volume\"], label=\"Volume\", alpha=0.6)\n",
    "plt.plot(daily_pd[\"date\"], daily_pd[\"ma_volume\"], label=\"7-Day MA Volume\", color=\"orange\")\n",
    "plt.title(\"Daily Volume and 7-Day Moving Average\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Volume\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "processed_df = df.drop(\"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\", \"taker_buy_quote_asset_volume\", \"open_time\")\n",
    "\n",
    "# Drop rows with nulls in critical columns\n",
    "processed_df = processed_df.dropna(subset=[\"open\", \"high\", \"low\", \"close\", \"volume\", \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"symbol\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = processed_df.withColumn(\"daily_return\", (col(\"close\") - col(\"open\")) / col(\"open\")) \\\n",
    "       .withColumn(\"volatility\", (col(\"high\") - col(\"low\")) / col(\"open\"))\n",
    "\n",
    "# Define window specification: partition by 'symbol' and order by 'date'\n",
    "processed_df = processed_df.repartition(\"symbol\").cache()\n",
    "processed_df.count()  # Forces cache population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first few rows of the processed DataFrame\n",
    "processed_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed DataFrame to S3 in Parquet format, partitioned by 'symbol'\n",
    "processed_df.write.mode(\"overwrite\").partitionBy(\"symbol\").parquet(\"s3a://cryptospark-dataset/daily_return_volatility/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the processed DataFrame from S3\n",
    "processed_df1 = spark.read.parquet(\"s3a://cryptospark-dataset/daily_return_volatility/\")\n",
    "\n",
    "# Repartition by symbol to improve parallelism for window operations\n",
    "processed_df1 = processed_df1.repartition(\"symbol\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative return\n",
    "#window_spec = Window.partitionBy(\"symbol\").orderBy(\"date\") \\\n",
    "    #.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Moving Averages (7-day and 30-day)\n",
    "window_7 = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-6, 0)\n",
    "window_30 = Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-29, 0)\n",
    "\n",
    "# Calculate cumulative return and moving averages\n",
    "processed_df1 = processed_df1.withColumn(\"ma_7\", avg(\"close\").over(window_7)) \\\n",
    "    .withColumn(\"ma_30\", avg(\"close\").over(window_30))\n",
    "    #.withColumn(\"cumulative_return\", exp(_sum(log(1 + col(\"daily_return\"))).over(window_spec)) - 1) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag features (previous day close price, volume)\n",
    "processed_df1 = processed_df1.withColumn(\"prev_close\", lag(\"close\", 1).over(window_7)) \\\n",
    "                             .withColumn(\"prev_volume\", lag(\"volume\", 1).over(window_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price change from previous day\n",
    "processed_df1 = processed_df1.withColumn(\"price_change\", col(\"close\") - col(\"prev_close\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent change from previous day\n",
    "processed_df1 = processed_df1.withColumn(\"percent_change\", \n",
    "    ((col(\"close\") - col(\"prev_close\")) / col(\"prev_close\")) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized volume (volume relative to 7-day average)\n",
    "processed_df1 = processed_df1.withColumn(\"volume_normalized\", \n",
    "    col(\"volume\") / avg(\"volume\").over(window_7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bollinger Bands: Compute 7-day std deviation and use it with MA_7\n",
    "processed_df1 = processed_df1.withColumn(\"stddev_7\", stddev(\"close\").over(window_7)) \\\n",
    "                             .withColumn(\"bollinger_upper\", col(\"ma_7\") + 2 * col(\"stddev_7\")) \\\n",
    "                             .withColumn(\"bollinger_lower\", col(\"ma_7\") - 2 * col(\"stddev_7\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the final DataFrame with new features\n",
    "processed_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df1 = processed_df1.repartition(\"symbol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new preprocessed data into s3\n",
    "processed_df1.write.mode(\"overwrite\").partitionBy(\"symbol\").parquet(\"s3a://cryptospark-dataset/processed-data/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
